{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557bd78a",
   "metadata": {},
   "source": [
    "# Notebook 2 : Chargement des Données vers BigQuery (LOAD)\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Ce notebook permet de **charger les données depuis Google Cloud Storage (GCS) vers BigQuery** pour créer la couche \"silver\" du pipeline ETL. \n",
    "\n",
    "Les données brutes stockées dans GCS (couche \"bronze\") sont chargées dans BigQuery.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir :\n",
    "\n",
    "1. **Exécuté le notebook `1_[EXTRACT]_ingest_to_gcs.ipynb`** pour avoir des données dans GCS\n",
    "2. **Fichier `.env` configuré** avec les variables d'environnement nécessaires\n",
    "3. **Service Account** avec les permissions BigQuery (`BigQuery Data Editor`, `BigQuery Job User`)\n",
    "4. **Packages Python installés** : `google-cloud-bigquery`, `google-cloud-storage`, `pandas`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962f99c",
   "metadata": {},
   "source": [
    "## 1 - Configuration et Authentification\n",
    "\n",
    "Cette section configure l'environnement et établit la connexion avec BigQuery et GCS.\n",
    "\n",
    "**Étapes :**\n",
    "- Import des bibliothèques nécessaires\n",
    "- Chargement des variables d'environnement depuis `.env`\n",
    "- Authentification avec le Service Account\n",
    "- Création des clients BigQuery et GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac220fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from src.bq_utils import (\n",
    "    load_csv_from_gcs,\n",
    "    load_parquet_from_gcs,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "SA_PATH = ROOT / os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "DATASET_ID = \"silver\"\n",
    "\n",
    "# Authentification\n",
    "creds = service_account.Credentials.from_service_account_file(SA_PATH)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=creds)\n",
    "storage_client = storage.Client(project=PROJECT_ID, credentials=creds)\n",
    "\n",
    "print(\"[OK] - Configuration et imports terminés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1044d",
   "metadata": {},
   "source": [
    "### 1.1 - Création du Dataset BigQuery\n",
    "\n",
    "Création du dataset \"silver\" s'il n'existe pas déjà. Le dataset est l'équivalent d'un schéma dans une base de données relationnelle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8781b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataset s'il n'existe pas\n",
    "dataset_ref = bq_client.dataset(DATASET_ID)\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"[OK] - Dataset {DATASET_ID} existe déjà\")\n",
    "except Exception:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = \"US\"\n",
    "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"[OK] - Dataset {DATASET_ID} créé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c77f9",
   "metadata": {},
   "source": [
    "## 2 - Chargement des Tables de Dimension\n",
    "\n",
    "Les tables de dimension contiennent les données de référence qui seront utilisées pour enrichir les tables de fait. Elles sont généralement stables dans le temps.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 - Table `dim_gare` (Emplacement des Gares)\n",
    "\n",
    "Cette table contient les informations géographiques et descriptives de toutes les gares d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Défini manuellement avec clé primaire `id_gares`\n",
    "- **Types de données** : Géographie (GEOGRAPHY), entiers, chaînes de caractères\n",
    "- **Clé primaire** : `id_gares` (mode REQUIRED)\n",
    "\n",
    "**Note** : Le schéma manuel permet de contrôler précisément les types de données, notamment pour les colonnes géographiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb26fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement direct depuis GCS (bronze) vers BigQuery (silver) avec schéma manuel\n",
    "gcs_path_gares = \"bronze/emplacement-des-gares-idf/emplacement-des-gares-idf.parquet\"\n",
    "\n",
    "# Définition du schéma manuel avec clé primaire (id_gares)\n",
    "schema_gares = [\n",
    "    bigquery.SchemaField(\"geo_point_2d\", \"GEOGRAPHY\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"geo_shape\", \"GEOGRAPHY\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_gares\", \"INTEGER\", mode=\"REQUIRED\", description=\"Clé primaire\"),\n",
    "    bigquery.SchemaField(\"nom_gares\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_so_gar\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_su_gar\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_ref_zdc\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_zdc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_ref_zda\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_zda\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idrefliga\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idrefligc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"res_com\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"indice_lig\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mode\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tertrain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"terrer\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"termetro\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tertram\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"terval\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"exploitant\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idf\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"principal\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"x\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"y\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"picto\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_iv\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "table_id = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_gares,\n",
    "    table_name=\"gares\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=schema_gares,\n",
    "    primary_key=\"id_gares\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df54d0",
   "metadata": {},
   "source": [
    "### 2.2 - Vérification de la Table `dim_gare`\n",
    "\n",
    "Après le chargement, on vérifie que les données ont été correctement chargées en :\n",
    "- Affichant le nombre de lignes\n",
    "- Listant les colonnes et leurs types\n",
    "- Afficant un aperçu des données (5 premières lignes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la table chargée\n",
    "table = bq_client.get_table(table_id)\n",
    "print(f\"[OK] - Nombre total de lignes: {table.num_rows}\")\n",
    "print(f\"[OK] - Colonnes:\")\n",
    "for field in table.schema:\n",
    "    print(f\"  - {field.name}: {field.field_type}\")\n",
    "\n",
    "# Requête simple pour vérifier les données et convertir en DataFrame pandas\n",
    "query = f\"SELECT * FROM `{table_id}` LIMIT 5\"\n",
    "results = bq_client.query(query).result()\n",
    "df = results.to_dataframe()\n",
    "\n",
    "print(f\"\\n[OK] - Aperçu des données (5 premières lignes):\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2937e",
   "metadata": {},
   "source": [
    "### 2.3 - Table `dim_ligne` (Référentiel des Lignes)\n",
    "\n",
    "Cette table contient les informations sur toutes les lignes de transport en commun d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les lignes (numéros, noms, types de transport, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd796e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path_lignes = \"bronze/referentiel-des-lignes/referentiel-des-lignes.parquet\"\n",
    "table_id_lignes = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_lignes,\n",
    "    table_name=\"dim_ligne\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a769c",
   "metadata": {},
   "source": [
    "### 2.4 - Table `dim_arret` (Référentiel des Arrêts)\n",
    "\n",
    "Cette table contient les informations sur tous les arrêts de transport en commun d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les arrêts (noms, coordonnées, lignes desservies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "gcs_path_arrets = \"bronze/arrets/arrets.parquet\"\n",
    "table_id_arrets = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_arrets,\n",
    "    table_name=\"dim_arret\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3adda",
   "metadata": {},
   "source": [
    "### 2.5 - Table `dim_transporteur` (Liste des Transporteurs)\n",
    "\n",
    "Cette table contient les informations sur tous les transporteurs (opérateurs de transport) d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les transporteurs (noms, codes, types de transport, etc.)\n",
    "\n",
    "**Note** : Cette table de dimension permet d'identifier les différents opérateurs de transport qui gèrent les lignes et arrêts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path_transporteurs = \"bronze/liste-transporteurs/liste-transporteurs.parquet\"\n",
    "table_id_transporteurs = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_transporteurs,\n",
    "    table_name=\"dim_transporteur\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9d463",
   "metadata": {},
   "source": [
    "### 2.5 - Table `dim_vacances_scolaires` (Calendrier des Vacances Scolaires)\n",
    "\n",
    "Cette table contient les périodes de vacances scolaires pour différentes zones et années.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : CSV (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Encodage** : UTF-8\n",
    "- **Séparateur** : Point-virgule (`;`)\n",
    "- **Contenu** : Dates de début/fin de vacances, zones, années, etc.\n",
    "\n",
    "**Note** : Pour les fichiers CSV, il est important de spécifier l'encodage et le séparateur pour éviter les erreurs de parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path_vacances = \"bronze/vacances-scolaires/vacances_scolaires.csv\"\n",
    "table_id_vacances = load_csv_from_gcs(\n",
    "    gcs_path=gcs_path_vacances,\n",
    "    table_name=\"dim_vacances_scolaires\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    skip_leading_rows=1,\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\";\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e9487",
   "metadata": {},
   "source": [
    "## 3 - Chargement des Tables de Fait\n",
    "\n",
    "Les tables de fait contiennent les mesures et événements métier. Ici, nous chargeons les données historiques de validations des titres de transport.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 - Configuration pour les Fichiers de Validation\n",
    "\n",
    "Les fichiers de validation historiques ont des formats différents selon les années :\n",
    "- **Encodages variés** : UTF-8, UTF-16LE, Latin-1\n",
    "- **Séparateurs variés** : Tabulation (`\\t`), point-virgule (`;`)\n",
    "- **Extensions variées** : `.txt`, `.csv`\n",
    "\n",
    "Ce dictionnaire de configuration permet de spécifier les paramètres corrects pour chaque fichier.\n",
    "\n",
    "**Note importante** : Le fichier `2023_S2_NB_FER.txt` utilise l'encodage UTF-16LE, qui n'est pas supporté directement par BigQuery. La fonction `load_csv_from_gcs` convertit automatiquement ce fichier en UTF-8 avant le chargement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d04b8",
   "metadata": {},
   "source": [
    "### 3.2 - Chargement des Fichiers de Validation\n",
    "\n",
    "Cette section charge tous les fichiers de validation historiques depuis GCS vers BigQuery.\n",
    "\n",
    "**Processus :**\n",
    "1. Parcourt le dictionnaire de configuration\n",
    "2. Recherche chaque fichier dans GCS\n",
    "3. Charge le fichier avec les paramètres appropriés (encodage, séparateur, format de date)\n",
    "4. Crée une table séparée pour chaque fichier (ex: `fact_validations_2015s1_nb_fer_csv`)\n",
    "\n",
    "**Gestion spéciale :**\n",
    "- **Fichiers UTF-16LE** : Conversion automatique en UTF-8 (nécessite `storage_client`)\n",
    "- **Format de date** : `DD/MM/YYYY` (format BigQuery pour les dates françaises)\n",
    "- **Schéma** : Auto-détecté (toutes les colonnes en STRING pour éviter les erreurs de parsing)\n",
    "\n",
    "**Durée estimée** : Plusieurs minutes selon le nombre et la taille des fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c38873",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rf_config = {\n",
    "    \"2015S1_NB_FER.csv\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2015S2_NB_FER.csv\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2016S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2016S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2017S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2017_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2018_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2019_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2019_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2020_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2020_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2021_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2021_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2022_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2022_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2023_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-16le\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2024_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e86c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger tous les fichiers de validation depuis GCS vers BigQuery\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Parcourir tous les fichiers dans la configuration\n",
    "for filename, config in load_rf_config.items():\n",
    "    # Chercher le fichier dans GCS\n",
    "    blobs = list(bucket.list_blobs(prefix=\"bronze/histo-validations-reseau-ferre/\"))\n",
    "    \n",
    "    # Trouver le blob correspondant\n",
    "    blob = None\n",
    "    for b in blobs:\n",
    "        if b.name.endswith(filename):\n",
    "            blob = b\n",
    "            break\n",
    "    \n",
    "    if blob is None:\n",
    "        print(f\"[SKIP] - {filename} (non trouvé dans GCS)\")\n",
    "        continue\n",
    "    \n",
    "    gcs_path = blob.name\n",
    "    table_name = f\"fact_validations_{filename.replace('.', '_').replace('-', '_').lower()}\"\n",
    "    \n",
    "    sep = config[\"sep\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "    \n",
    "    # Convertir \"\\t\" en tabulation réelle si nécessaire\n",
    "    if sep == \"\\\\t\":\n",
    "        sep = \"\\t\"\n",
    "    \n",
    "    # Utiliser la fonction load_csv_from_gcs avec le schéma unifié (toutes les colonnes en STRING)\n",
    "    table_id = load_csv_from_gcs(\n",
    "        gcs_path=gcs_path,\n",
    "        table_name=table_name,\n",
    "        bq_client=bq_client,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        schema=None,\n",
    "        skip_leading_rows=config.get(\"skip_rows\", 1),\n",
    "        encoding=encoding,\n",
    "        sep=sep,\n",
    "        date_format=\"DD/MM/YYYY\",  # Format BigQuery pour les dates\n",
    "        storage_client=storage_client,  # Requis pour la conversion UTF-16LE\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl-gcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557bd78a",
   "metadata": {},
   "source": [
    "# Notebook 2 : Chargement des Données vers BigQuery (LOAD)\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Ce notebook permet de **charger les données depuis Google Cloud Storage (GCS) vers BigQuery** pour créer la couche \"silver\" du pipeline ETL. \n",
    "\n",
    "Les données brutes stockées dans GCS (couche \"bronze\") sont chargées dans BigQuery où elles seront transformées et structurées selon un schéma en étoile (star schema).\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir :\n",
    "\n",
    "1. **Exécuté le notebook `1_[EXTRACT]_ingest_to_gcs.ipynb`** pour avoir des données dans GCS\n",
    "2. **Fichier `.env` configuré** avec les variables d'environnement nécessaires\n",
    "3. **Service Account** avec les permissions BigQuery (`BigQuery Data Editor`, `BigQuery Job User`)\n",
    "4. **Packages Python installés** : `google-cloud-bigquery`, `google-cloud-storage`, `pandas`, etc.\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "1. **Configuration et Authentification** : Connexion à BigQuery et création du dataset \"silver\"\n",
    "2. **Chargement des Tables de Dimension** :\n",
    "   - `dim_gare` : Emplacement des gares (schéma manuel avec clé primaire)\n",
    "   - `dim_ligne` : Référentiel des lignes\n",
    "   - `dim_arret` : Référentiel des arrêts\n",
    "   - `dim_vacances_scolaires` : Calendrier des vacances scolaires\n",
    "   - `dim_transporteur` : Liste des transporteurs (opérateurs de transport)\n",
    "3. **Chargement des Tables de Fait** :\n",
    "   - `fact_validations` : Données historiques de validations (plusieurs fichiers)\n",
    "\n",
    "## Résultats Attendus\n",
    "\n",
    "À la fin de l'exécution, vous devriez avoir :\n",
    "- Un dataset BigQuery nommé `silver` contenant toutes les tables\n",
    "- Des tables de dimension pour les analyses\n",
    "- Des tables de fait avec les données de validations historiques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962f99c",
   "metadata": {},
   "source": [
    "## 1 - Configuration et Authentification\n",
    "\n",
    "Cette section configure l'environnement et établit la connexion avec BigQuery et GCS.\n",
    "\n",
    "**Étapes :**\n",
    "- Import des bibliothèques nécessaires\n",
    "- Chargement des variables d'environnement depuis `.env`\n",
    "- Authentification avec le Service Account\n",
    "- Création des clients BigQuery et GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac220fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] - Configuration et imports terminés\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from src.bq_utils import (\n",
    "    load_csv_from_gcs,\n",
    "    load_parquet_from_gcs,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "SA_PATH = ROOT / os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "DATASET_ID = \"silver\"\n",
    "\n",
    "# Authentification\n",
    "creds = service_account.Credentials.from_service_account_file(SA_PATH)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=creds)\n",
    "storage_client = storage.Client(project=PROJECT_ID, credentials=creds)\n",
    "\n",
    "print(\"[OK] - Configuration et imports terminés\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1044d",
   "metadata": {},
   "source": [
    "### 1.1 - Création du Dataset BigQuery\n",
    "\n",
    "Création du dataset \"silver\" s'il n'existe pas déjà. Le dataset est l'équivalent d'un schéma dans une base de données relationnelle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8781b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] - Dataset silver existe déjà\n"
     ]
    }
   ],
   "source": [
    "# Création du dataset s'il n'existe pas\n",
    "dataset_ref = bq_client.dataset(DATASET_ID)\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"[OK] - Dataset {DATASET_ID} existe déjà\")\n",
    "except Exception:\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = \"US\"\n",
    "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"[OK] - Dataset {DATASET_ID} créé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c77f9",
   "metadata": {},
   "source": [
    "## 2 - Chargement des Tables de Dimension\n",
    "\n",
    "Les tables de dimension contiennent les données de référence qui seront utilisées pour enrichir les tables de fait. Elles sont généralement stables dans le temps.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 - Table `dim_gare` (Emplacement des Gares)\n",
    "\n",
    "Cette table contient les informations géographiques et descriptives de toutes les gares d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Défini manuellement avec clé primaire `id_gares`\n",
    "- **Types de données** : Géographie (GEOGRAPHY), entiers, chaînes de caractères\n",
    "- **Clé primaire** : `id_gares` (mode REQUIRED)\n",
    "\n",
    "**Note** : Le schéma manuel permet de contrôler précisément les types de données, notamment pour les colonnes géographiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb26fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement direct depuis GCS (bronze) vers BigQuery (silver) avec schéma manuel\n",
    "gcs_path_gares = \"bronze/emplacement-des-gares-idf/emplacement-des-gares-idf.parquet\"\n",
    "\n",
    "# Définition du schéma manuel avec clé primaire (id_gares)\n",
    "schema_gares = [\n",
    "    bigquery.SchemaField(\"geo_point_2d\", \"GEOGRAPHY\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"geo_shape\", \"GEOGRAPHY\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_gares\", \"INTEGER\", mode=\"REQUIRED\", description=\"Clé primaire\"),\n",
    "    bigquery.SchemaField(\"nom_gares\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_so_gar\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_su_gar\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_ref_zdc\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_zdc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_ref_zda\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_zda\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idrefliga\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idrefligc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"res_com\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"indice_lig\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mode\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tertrain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"terrer\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"termetro\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tertram\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"terval\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"exploitant\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"idf\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"principal\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"x\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"y\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"picto\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"nom_iv\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "table_id = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_gares,\n",
    "    table_name=\"dim_gare\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=schema_gares,\n",
    "    primary_key=\"id_gares\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df54d0",
   "metadata": {},
   "source": [
    "### 2.2 - Vérification de la Table `dim_gare`\n",
    "\n",
    "Après le chargement, on vérifie que les données ont été correctement chargées en :\n",
    "- Affichant le nombre de lignes\n",
    "- Listant les colonnes et leurs types\n",
    "- Afficant un aperçu des données (5 premières lignes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6688f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] - Nombre total de lignes: 1234\n",
      "[OK] - Colonnes:\n",
      "  - geo_point_2d: GEOGRAPHY\n",
      "  - geo_shape: GEOGRAPHY\n",
      "  - id_gares: INTEGER\n",
      "  - nom_gares: STRING\n",
      "  - nom_so_gar: STRING\n",
      "  - nom_su_gar: STRING\n",
      "  - id_ref_zdc: INTEGER\n",
      "  - nom_zdc: STRING\n",
      "  - id_ref_zda: INTEGER\n",
      "  - nom_zda: STRING\n",
      "  - idrefliga: STRING\n",
      "  - idrefligc: STRING\n",
      "  - res_com: STRING\n",
      "  - indice_lig: STRING\n",
      "  - mode: STRING\n",
      "  - tertrain: STRING\n",
      "  - terrer: STRING\n",
      "  - termetro: STRING\n",
      "  - tertram: STRING\n",
      "  - terval: STRING\n",
      "  - exploitant: STRING\n",
      "  - idf: INTEGER\n",
      "  - principal: INTEGER\n",
      "  - x: FLOAT\n",
      "  - y: FLOAT\n",
      "  - picto: STRING\n",
      "  - nom_iv: STRING\n",
      "\n",
      "[OK] - Aperçu des données (5 premières lignes):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/.pyenv/versions/3.11.14/envs/etl-gcp/lib/python3.11/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_point_2d</th>\n",
       "      <th>geo_shape</th>\n",
       "      <th>id_gares</th>\n",
       "      <th>nom_gares</th>\n",
       "      <th>nom_so_gar</th>\n",
       "      <th>nom_su_gar</th>\n",
       "      <th>id_ref_zdc</th>\n",
       "      <th>nom_zdc</th>\n",
       "      <th>id_ref_zda</th>\n",
       "      <th>nom_zda</th>\n",
       "      <th>...</th>\n",
       "      <th>termetro</th>\n",
       "      <th>tertram</th>\n",
       "      <th>terval</th>\n",
       "      <th>exploitant</th>\n",
       "      <th>idf</th>\n",
       "      <th>principal</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>picto</th>\n",
       "      <th>nom_iv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT(2.57064006889043 49.0100449600405)</td>\n",
       "      <td>POINT(2.57064006889043 49.0100449600405)</td>\n",
       "      <td>1004</td>\n",
       "      <td>Parc PX</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>73595</td>\n",
       "      <td>NC</td>\n",
       "      <td>52244</td>\n",
       "      <td>Parc Px</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Transdev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>668589.1929</td>\n",
       "      <td>6.878989e+06</td>\n",
       "      <td>None</td>\n",
       "      <td>Parc PX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT(2.54483189791712 49.0089784609502)</td>\n",
       "      <td>POINT(2.54483189791712 49.0089784609502)</td>\n",
       "      <td>1002</td>\n",
       "      <td>Parc PR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>74162</td>\n",
       "      <td>NC</td>\n",
       "      <td>59080</td>\n",
       "      <td>Parc Pr</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Transdev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>666700.4738</td>\n",
       "      <td>6.878881e+06</td>\n",
       "      <td>None</td>\n",
       "      <td>Parc PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT(2.56055884390188 49.0100018661511)</td>\n",
       "      <td>POINT(2.56055884390188 49.0100018661511)</td>\n",
       "      <td>1003</td>\n",
       "      <td>Terminal 3 - Roissypole</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>73596</td>\n",
       "      <td>Aéroport CDG 1 (Terminal 3)</td>\n",
       "      <td>462398</td>\n",
       "      <td>Aéroport CDG 1 (Terminal 3) - RER</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Transdev</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>667851.6587</td>\n",
       "      <td>6.878988e+06</td>\n",
       "      <td>None</td>\n",
       "      <td>Terminal 3 - Roissypole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT(2.34266003733635 48.8846809737278)</td>\n",
       "      <td>POINT(2.34266003733635 48.8846809737278)</td>\n",
       "      <td>1017</td>\n",
       "      <td>Funiculaire Montmartre Station Basse</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>73849</td>\n",
       "      <td>Funiculaire de Montmartre - Gare basse</td>\n",
       "      <td>45559</td>\n",
       "      <td>Funiculaire de Montmartre - Gare basse</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FUNICULAIRE MONTMART</td>\n",
       "      <td>RATP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>651795.0575</td>\n",
       "      <td>6.865163e+06</td>\n",
       "      <td>None</td>\n",
       "      <td>Funiculaire Montmartre Station Basse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT(2.34254965325849 48.8856611127015)</td>\n",
       "      <td>POINT(2.34254965325849 48.8856611127015)</td>\n",
       "      <td>1018</td>\n",
       "      <td>Funiculaire Montmartre Station Haute</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>73848</td>\n",
       "      <td>Funiculaire de Montmartre - Gare haute</td>\n",
       "      <td>45558</td>\n",
       "      <td>Funiculaire de Montmartre - Gare haute</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FUNICULAIRE MONTMART</td>\n",
       "      <td>RATP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>651787.8703</td>\n",
       "      <td>6.865272e+06</td>\n",
       "      <td>None</td>\n",
       "      <td>Funiculaire Montmartre Station Haute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               geo_point_2d  \\\n",
       "0  POINT(2.57064006889043 49.0100449600405)   \n",
       "1  POINT(2.54483189791712 49.0089784609502)   \n",
       "2  POINT(2.56055884390188 49.0100018661511)   \n",
       "3  POINT(2.34266003733635 48.8846809737278)   \n",
       "4  POINT(2.34254965325849 48.8856611127015)   \n",
       "\n",
       "                                  geo_shape  id_gares  \\\n",
       "0  POINT(2.57064006889043 49.0100449600405)      1004   \n",
       "1  POINT(2.54483189791712 49.0089784609502)      1002   \n",
       "2  POINT(2.56055884390188 49.0100018661511)      1003   \n",
       "3  POINT(2.34266003733635 48.8846809737278)      1017   \n",
       "4  POINT(2.34254965325849 48.8856611127015)      1018   \n",
       "\n",
       "                              nom_gares nom_so_gar nom_su_gar  id_ref_zdc  \\\n",
       "0                               Parc PX       None       None       73595   \n",
       "1                               Parc PR       None       None       74162   \n",
       "2               Terminal 3 - Roissypole       None       None       73596   \n",
       "3  Funiculaire Montmartre Station Basse       None       None       73849   \n",
       "4  Funiculaire Montmartre Station Haute       None       None       73848   \n",
       "\n",
       "                                  nom_zdc  id_ref_zda  \\\n",
       "0                                      NC       52244   \n",
       "1                                      NC       59080   \n",
       "2             Aéroport CDG 1 (Terminal 3)      462398   \n",
       "3  Funiculaire de Montmartre - Gare basse       45559   \n",
       "4  Funiculaire de Montmartre - Gare haute       45558   \n",
       "\n",
       "                                  nom_zda  ... termetro tertram  \\\n",
       "0                                 Parc Px  ...        0       0   \n",
       "1                                 Parc Pr  ...        0       0   \n",
       "2       Aéroport CDG 1 (Terminal 3) - RER  ...        0       0   \n",
       "3  Funiculaire de Montmartre - Gare basse  ...        0       0   \n",
       "4  Funiculaire de Montmartre - Gare haute  ...        0       0   \n",
       "\n",
       "                 terval exploitant idf principal            x             y  \\\n",
       "0                     0   Transdev   1         0  668589.1929  6.878989e+06   \n",
       "1                     0   Transdev   1         0  666700.4738  6.878881e+06   \n",
       "2                     0   Transdev   1         0  667851.6587  6.878988e+06   \n",
       "3  FUNICULAIRE MONTMART       RATP   1         0  651795.0575  6.865163e+06   \n",
       "4  FUNICULAIRE MONTMART       RATP   1         0  651787.8703  6.865272e+06   \n",
       "\n",
       "  picto                                nom_iv  \n",
       "0  None                               Parc PX  \n",
       "1  None                               Parc PR  \n",
       "2  None               Terminal 3 - Roissypole  \n",
       "3  None  Funiculaire Montmartre Station Basse  \n",
       "4  None  Funiculaire Montmartre Station Haute  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vérification de la table chargée\n",
    "table = bq_client.get_table(table_id)\n",
    "print(f\"[OK] - Nombre total de lignes: {table.num_rows}\")\n",
    "print(f\"[OK] - Colonnes:\")\n",
    "for field in table.schema:\n",
    "    print(f\"  - {field.name}: {field.field_type}\")\n",
    "\n",
    "# Requête simple pour vérifier les données et convertir en DataFrame pandas\n",
    "query = f\"SELECT * FROM `{table_id}` LIMIT 5\"\n",
    "results = bq_client.query(query).result()\n",
    "df = results.to_dataframe()\n",
    "\n",
    "print(f\"\\n[OK] - Aperçu des données (5 premières lignes):\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2937e",
   "metadata": {},
   "source": [
    "### 2.3 - Table `dim_ligne` (Référentiel des Lignes)\n",
    "\n",
    "Cette table contient les informations sur toutes les lignes de transport en commun d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les lignes (numéros, noms, types de transport, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd796e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/referentiel-des-lignes/referentiel-des-lignes.parquet vers univ-reims-sncf-forecast.silver.dim_ligne...\n",
      "[OK] - 2120 lignes chargées dans univ-reims-sncf-forecast.silver.dim_ligne\n",
      "[OK] - Taille: 0.56 MB\n"
     ]
    }
   ],
   "source": [
    "gcs_path_lignes = \"bronze/referentiel-des-lignes/referentiel-des-lignes.parquet\"\n",
    "table_id_lignes = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_lignes,\n",
    "    table_name=\"dim_ligne\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a769c",
   "metadata": {},
   "source": [
    "### 2.4 - Table `dim_arret` (Référentiel des Arrêts)\n",
    "\n",
    "Cette table contient les informations sur tous les arrêts de transport en commun d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les arrêts (noms, coordonnées, lignes desservies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e16ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/arrets/arrets.parquet vers univ-reims-sncf-forecast.silver.dim_arret...\n",
      "[OK] - 38619 lignes chargées dans univ-reims-sncf-forecast.silver.dim_arret\n",
      "[OK] - Taille: 6.34 MB\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "gcs_path_arrets = \"bronze/arrets/arrets.parquet\"\n",
    "table_id_arrets = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_arrets,\n",
    "    table_name=\"dim_arret\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec3cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd4e9487",
   "metadata": {},
   "source": [
    "## 3 - Chargement des Tables de Fait\n",
    "\n",
    "Les tables de fait contiennent les mesures et événements métier. Ici, nous chargeons les données historiques de validations des titres de transport.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 - Configuration pour les Fichiers de Validation\n",
    "\n",
    "Les fichiers de validation historiques ont des formats différents selon les années :\n",
    "- **Encodages variés** : UTF-8, UTF-16LE, Latin-1\n",
    "- **Séparateurs variés** : Tabulation (`\\t`), point-virgule (`;`)\n",
    "- **Extensions variées** : `.txt`, `.csv`\n",
    "\n",
    "Ce dictionnaire de configuration permet de spécifier les paramètres corrects pour chaque fichier.\n",
    "\n",
    "**Note importante** : Le fichier `2023_S2_NB_FER.txt` utilise l'encodage UTF-16LE, qui n'est pas supporté directement par BigQuery. La fonction `load_csv_from_gcs` convertit automatiquement ce fichier en UTF-8 avant le chargement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7f32e",
   "metadata": {},
   "source": [
    "### 2.5 - Table `dim_vacances_scolaires` (Calendrier des Vacances Scolaires)\n",
    "\n",
    "Cette table contient les périodes de vacances scolaires pour différentes zones et années.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : CSV (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Encodage** : UTF-8\n",
    "- **Séparateur** : Point-virgule (`;`)\n",
    "- **Contenu** : Dates de début/fin de vacances, zones, années, etc.\n",
    "\n",
    "**Note** : Pour les fichiers CSV, il est important de spécifier l'encodage et le séparateur pour éviter les erreurs de parsing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d04b8",
   "metadata": {},
   "source": [
    "### 3.2 - Chargement des Fichiers de Validation\n",
    "\n",
    "Cette section charge tous les fichiers de validation historiques depuis GCS vers BigQuery.\n",
    "\n",
    "**Processus :**\n",
    "1. Parcourt le dictionnaire de configuration\n",
    "2. Recherche chaque fichier dans GCS\n",
    "3. Charge le fichier avec les paramètres appropriés (encodage, séparateur, format de date)\n",
    "4. Crée une table séparée pour chaque fichier (ex: `fact_validations_2015s1_nb_fer_csv`)\n",
    "\n",
    "**Gestion spéciale :**\n",
    "- **Fichiers UTF-16LE** : Conversion automatique en UTF-8 (nécessite `storage_client`)\n",
    "- **Format de date** : `DD/MM/YYYY` (format BigQuery pour les dates françaises)\n",
    "- **Schéma** : Auto-détecté (toutes les colonnes en STRING pour éviter les erreurs de parsing)\n",
    "\n",
    "**Durée estimée** : Plusieurs minutes selon le nombre et la taille des fichiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c28050",
   "metadata": {},
   "source": [
    "## 4 - Vérification et Résumé\n",
    "\n",
    "### Vérification des Tables Créées\n",
    "\n",
    "Après le chargement, vous pouvez vérifier que toutes les tables ont été créées dans BigQuery :\n",
    "\n",
    "```python\n",
    "# Lister toutes les tables du dataset \"silver\"\n",
    "tables = list(bq_client.list_tables(f\"{PROJECT_ID}.{DATASET_ID}\"))\n",
    "print(f\"Nombre total de tables : {len(tables)}\")\n",
    "for table in tables:\n",
    "    table_ref = bq_client.get_table(table)\n",
    "    print(f\"  - {table.table_id}: {table_ref.num_rows} lignes\")\n",
    "```\n",
    "\n",
    "### Prochaines Étapes\n",
    "\n",
    "Une fois les données chargées dans BigQuery, vous pouvez :\n",
    "1. **Explorer les données** avec des requêtes SQL dans BigQuery\n",
    "2. **Créer des vues** pour faciliter les analyses\n",
    "3. **Transformer les données** selon vos besoins métier\n",
    "4. **Créer des visualisations** avec des outils comme Data Studio ou Looker\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Erreur \"File not found\"** :\n",
    "- Vérifiez que le notebook `1_[EXTRACT]_ingest_to_gcs.ipynb` a été exécuté\n",
    "- Vérifiez que les fichiers existent dans GCS avec le bon chemin\n",
    "\n",
    "**Erreur \"Permission denied\"** :\n",
    "- Vérifiez les permissions du Service Account\n",
    "- Assurez-vous que le Service Account a les rôles `BigQuery Data Editor` et `BigQuery Job User`\n",
    "\n",
    "**Erreur d'encodage** :\n",
    "- Vérifiez que l'encodage spécifié dans la configuration correspond au fichier\n",
    "- Pour UTF-16LE, assurez-vous que `storage_client` est passé à la fonction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf80dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/vacances-scolaires/vacances_scolaires.csv vers univ-reims-sncf-forecast.silver.dim_vacances_scolaires...\n",
      "[OK] - 2306 lignes chargées dans univ-reims-sncf-forecast.silver.dim_vacances_scolaires\n",
      "[OK] - Taille: 0.16 MB\n"
     ]
    }
   ],
   "source": [
    "gcs_path_vacances = \"bronze/vacances-scolaires/vacances_scolaires.csv\"\n",
    "table_id_vacances = load_csv_from_gcs(\n",
    "    gcs_path=gcs_path_vacances,\n",
    "    table_name=\"dim_vacances_scolaires\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    skip_leading_rows=1,\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\";\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2123f",
   "metadata": {},
   "source": [
    "### 2.6 - Table `dim_transporteur` (Liste des Transporteurs)\n",
    "\n",
    "Cette table contient les informations sur tous les transporteurs (opérateurs de transport) d'Île-de-France.\n",
    "\n",
    "**Caractéristiques :**\n",
    "- **Format source** : Parquet (depuis GCS)\n",
    "- **Schéma** : Auto-détecté par BigQuery\n",
    "- **Contenu** : Informations sur les transporteurs (noms, codes, types de transport, etc.)\n",
    "\n",
    "**Note** : Cette table de dimension permet d'identifier les différents opérateurs de transport qui gèrent les lignes et arrêts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a3b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/liste-transporteurs/liste-transporteurs.parquet vers univ-reims-sncf-forecast.silver.dim_transporteur...\n",
      "[OK] - 53 lignes chargées dans univ-reims-sncf-forecast.silver.dim_transporteur\n",
      "[OK] - Taille: 0.01 MB\n"
     ]
    }
   ],
   "source": [
    "gcs_path_transporteurs = \"bronze/liste-transporteurs/liste-transporteurs.parquet\"\n",
    "table_id_transporteurs = load_parquet_from_gcs(\n",
    "    gcs_path=gcs_path_transporteurs,\n",
    "    table_name=\"dim_transporteur\",\n",
    "    bq_client=bq_client,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    schema=None,  # Autodetect\n",
    "    primary_key=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c05ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c38873",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rf_config = {\n",
    "    \"2015S1_NB_FER.csv\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2015S2_NB_FER.csv\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2016S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2016S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2017S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2017_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2018_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2019_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2019_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2020_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2020_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2021_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2021_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2022_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2022_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \";\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2023_S2_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-16le\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    },\n",
    "    \"2024_S1_NB_FER.txt\": {\n",
    "        \"encoding\": \"utf-8\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"skip_rows\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e86c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2015/data-rf-2015/2015S1_NB_FER.csv vers univ-reims-sncf-forecast.silver.fact_validations_2015s1_nb_fer_csv...\n",
      "[OK] - 755989 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2015s1_nb_fer_csv\n",
      "[OK] - Taille: 45.93 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2015/data-rf-2015/2015S2_NB_FER.csv vers univ-reims-sncf-forecast.silver.fact_validations_2015s2_nb_fer_csv...\n",
      "[OK] - 778747 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2015s2_nb_fer_csv\n",
      "[OK] - Taille: 47.28 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2016/data-rf-2016/2016S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2016s1_nb_fer_txt...\n",
      "[OK] - 779712 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2016s1_nb_fer_txt\n",
      "[OK] - Taille: 52.96 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2016/data-rf-2016/2016S2_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2016s2_nb_fer_txt...\n",
      "[OK] - 774421 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2016s2_nb_fer_txt\n",
      "[OK] - Taille: 52.59 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2017/data-rf-2017/2017S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2017s1_nb_fer_txt...\n",
      "[OK] - 780270 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2017s1_nb_fer_txt\n",
      "[OK] - Taille: 53.02 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2017/data-rf-2017/2017_S2_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2017_s2_nb_fer_txt...\n",
      "[OK] - 825698 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2017_s2_nb_fer_txt\n",
      "[OK] - Taille: 52.00 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2018/data-rf-2018/2018_S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2018_s1_nb_fer_txt...\n",
      "[OK] - 866694 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2018_s1_nb_fer_txt\n",
      "[OK] - Taille: 54.94 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2019/data-rf-2019/2019_S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2019_s1_nb_fer_txt...\n",
      "[OK] - 934851 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2019_s1_nb_fer_txt\n",
      "[OK] - Taille: 59.47 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2019/data-rf-2019/2019_S2_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2019_s2_nb_fer_txt...\n",
      "[OK] - 953454 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2019_s2_nb_fer_txt\n",
      "[OK] - Taille: 60.98 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2020/data-rf-2020/2020_S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2020_s1_nb_fer_txt...\n",
      "[OK] - 887493 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2020_s1_nb_fer_txt\n",
      "[OK] - Taille: 56.89 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2020/data-rf-2020/2020_S2_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2020_s2_nb_fer_txt...\n",
      "[OK] - 1054012 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2020_s2_nb_fer_txt\n",
      "[OK] - Taille: 67.52 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2021/data-rf-2021/2021_S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2021_s1_nb_fer_txt...\n",
      "[OK] - 1064019 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2021_s1_nb_fer_txt\n",
      "[OK] - Taille: 68.20 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2021/data-rf-2021/2021_S2_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2021_s2_nb_fer_txt...\n",
      "[OK] - 1084281 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2021_s2_nb_fer_txt\n",
      "[OK] - Taille: 69.54 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2022/data-rf-2022/2022_S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2022_s1_nb_fer_txt...\n",
      "[OK] - 1088334 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2022_s1_nb_fer_txt\n",
      "[OK] - Taille: 69.80 MB\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2022/data-rf-2022/2022_S2_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2022_s2_nb_fer_txt...\n",
      "[OK] - 1105947 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2022_s2_nb_fer_txt\n",
      "[OK] - Taille: 69.08 MB\n",
      "\n",
      "[...] - Conversion UTF-16LE -> UTF-8 pour bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt...\n",
      "[OK] - Fichier converti et uploadé vers bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt.utf8\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt.utf8 vers univ-reims-sncf-forecast.silver.fact_validations_2023_s2_nb_fer_txt...\n",
      "[OK] - 849596 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2023_s2_nb_fer_txt\n",
      "[OK] - Taille: 62.86 MB\n",
      "[OK] - Fichier temporaire bronze/histo-validations-reseau-ferre/2023/data-rf-2023/2023_S2_NB_FER.txt.utf8 supprimé\n",
      "\n",
      "[...] - Chargement de gs://bronze-sncf-etl/bronze/histo-validations-reseau-ferre/2024/2024_S1_NB_FER.txt vers univ-reims-sncf-forecast.silver.fact_validations_2024_s1_nb_fer_txt...\n",
      "[OK] - 859043 lignes chargées dans univ-reims-sncf-forecast.silver.fact_validations_2024_s1_nb_fer_txt\n",
      "[OK] - Taille: 63.60 MB\n"
     ]
    }
   ],
   "source": [
    "# Charger tous les fichiers de validation depuis GCS vers BigQuery\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "# Parcourir tous les fichiers dans la configuration\n",
    "for filename, config in load_rf_config.items():\n",
    "    # Chercher le fichier dans GCS\n",
    "    blobs = list(bucket.list_blobs(prefix=\"bronze/histo-validations-reseau-ferre/\"))\n",
    "    \n",
    "    # Trouver le blob correspondant\n",
    "    blob = None\n",
    "    for b in blobs:\n",
    "        if b.name.endswith(filename):\n",
    "            blob = b\n",
    "            break\n",
    "    \n",
    "    if blob is None:\n",
    "        print(f\"[SKIP] - {filename} (non trouvé dans GCS)\")\n",
    "        continue\n",
    "    \n",
    "    gcs_path = blob.name\n",
    "    table_name = f\"fact_validations_{filename.replace('.', '_').replace('-', '_').lower()}\"\n",
    "    \n",
    "    sep = config[\"sep\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "    \n",
    "    # Convertir \"\\t\" en tabulation réelle si nécessaire\n",
    "    if sep == \"\\\\t\":\n",
    "        sep = \"\\t\"\n",
    "    \n",
    "    # Utiliser la fonction load_csv_from_gcs avec le schéma unifié (toutes les colonnes en STRING)\n",
    "    table_id = load_csv_from_gcs(\n",
    "        gcs_path=gcs_path,\n",
    "        table_name=table_name,\n",
    "        bq_client=bq_client,\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        schema=None,\n",
    "        skip_leading_rows=config.get(\"skip_rows\", 1),\n",
    "        encoding=encoding,\n",
    "        sep=sep,\n",
    "        date_format=\"DD/MM/YYYY\",  # Format BigQuery pour les dates\n",
    "        storage_client=storage_client,  # Requis pour la conversion UTF-16LE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269271c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl-gcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3 : Analyse pour la Construction de la Couche Gold\n",
        "\n",
        "## Objectif\n",
        "\n",
        "Ce notebook vous guide dans l'**analyse des donn√©es de la couche silver** pour pr√©parer la construction de la couche **gold** (analytics/BI).\n",
        "\n",
        "La couche gold contient des donn√©es transform√©es, nettoy√©es et optimis√©es pour l'analyse m√©tier et la cr√©ation de rapports/dashboards.\n",
        "\n",
        "## Pr√©requis\n",
        "\n",
        "Avant d'ex√©cuter ce notebook, assurez-vous d'avoir :\n",
        "\n",
        "1. **Ex√©cut√© le notebook `2_[LOAD]_load_to_bigquery.ipynb`** pour avoir toutes les tables dans BigQuery (dataset `silver`)\n",
        "2. **Fichier `.env` configur√©** avec les variables d'environnement n√©cessaires\n",
        "3. **Service Account** avec les permissions BigQuery (`BigQuery Data Viewer`, `BigQuery Job User`)\n",
        "4. **Packages Python install√©s** : `google-cloud-bigquery`, `pandas`, `db-dtypes`, etc.\n",
        "\n",
        "## Structure du Notebook\n",
        "\n",
        "Ce notebook contient **4 t√¢ches principales** √† r√©aliser :\n",
        "\n",
        "1. **T√¢che 1 : Analyser la Granularit√©** - Comprendre le niveau de d√©tail de chaque table\n",
        "2. **T√¢che 2 : Identifier les Transformations** - D√©terminer les transformations n√©cessaires pour la couche gold\n",
        "3. **T√¢che 3 : Identifier les Cl√©s de Jointure** - Mapper les relations entre les tables\n",
        "4. **T√¢che 4 : Autres Analyses** - Qualit√© des donn√©es, agr√©gations possibles, etc.\n",
        "\n",
        "## R√©sultats Attendus\n",
        "\n",
        "√Ä la fin de ce notebook, vous devriez avoir :\n",
        "- Une compr√©hension claire de la structure et de la granularit√© de chaque table\n",
        "- Une liste des transformations √† appliquer pour cr√©er la couche gold\n",
        "- Un sch√©ma de jointures document√©\n",
        "- Des recommandations pour l'optimisation et l'agr√©gation des donn√©es\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration et Connexion √† BigQuery\n",
        "\n",
        "Cette section configure l'environnement et √©tablit la connexion avec BigQuery pour explorer les donn√©es de la couche silver.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# Configuration\n",
        "load_dotenv()\n",
        "\n",
        "ROOT = Path.cwd().parent\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
        "SA_PATH = ROOT / os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "DATASET_ID = \"silver\"\n",
        "\n",
        "# Authentification\n",
        "creds = service_account.Credentials.from_service_account_file(SA_PATH)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID, credentials=creds)\n",
        "\n",
        "print(f\"[OK] - Connect√© au projet: {PROJECT_ID}\")\n",
        "print(f\"[OK] - Dataset: {DATASET_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## T√¢che 1 : Analyser la Granularit√© de Chaque Table\n",
        "\n",
        "### Objectif\n",
        "\n",
        "La **granularit√©** d'une table correspond au niveau de d√©tail des donn√©es qu'elle contient. Comprendre la granularit√© est essentiel pour :\n",
        "- D√©terminer comment agr√©ger les donn√©es\n",
        "- Identifier les duplications potentielles\n",
        "- Comprendre le niveau de d√©tail n√©cessaire pour les analyses m√©tier\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Pour chaque table du dataset `silver`, vous devez :\n",
        "\n",
        "1. **Lister les colonnes** et leurs types\n",
        "2. **Identifier les cl√©s primaires** ou les colonnes qui identifient de mani√®re unique une ligne\n",
        "3. **D√©terminer la granularit√©** : √† quel niveau de d√©tail sont les donn√©es ?\n",
        "   - Exemple : `fact_validations` pourrait √™tre au niveau **jour √ó gare √ó type de titre**\n",
        "4. **Compter les lignes** et estimer la taille des donn√©es\n",
        "5. **Identifier les colonnes de dimension** (r√©f√©rences vers d'autres tables)\n",
        "\n",
        "### Exemple de Format de R√©ponse\n",
        "\n",
        "```\n",
        "Table: dim_gare\n",
        "- Granularit√©: 1 ligne = 1 gare\n",
        "- Cl√© primaire: id_gares\n",
        "- Nombre de lignes: 1234\n",
        "- Colonnes de dimension: aucune (table de dimension)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### √Ä Compl√©ter : Analyse de Granularit√©\n",
        "\n",
        "**Tables de Dimension :**\n",
        "\n",
        "1. `dim_gare`\n",
        "2. `dim_ligne`\n",
        "3. `dim_arret`\n",
        "4. `dim_vacances_scolaires`\n",
        "5. `dim_transporteur`\n",
        "\n",
        "**Tables de Fait :**\n",
        "\n",
        "6. `fact_validations_*` (toutes les tables de validation)\n",
        "\n",
        "**Votre t√¢che :** Ex√©cutez des requ√™tes SQL pour analyser chaque table et remplir le tableau ci-dessous.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple : Analyser la table dim_gare\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    COUNT(*) as nb_lignes,\n",
        "    COUNT(DISTINCT id_gares) as nb_gares_uniques,\n",
        "    COUNT(*) - COUNT(id_gares) as nb_id_null\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.dim_gare`\n",
        "\"\"\"\n",
        "\n",
        "df = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Analyse de dim_gare ===\")\n",
        "display(df)\n",
        "\n",
        "# Afficher le sch√©ma\n",
        "table = bq_client.get_table(f\"{PROJECT_ID}.{DATASET_ID}.dim_gare\")\n",
        "print(\"\\n=== Sch√©ma ===\")\n",
        "for field in table.schema:\n",
        "    print(f\"  - {field.name}: {field.field_type} ({field.mode})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : R√©p√©tez cette analyse pour toutes les tables\n",
        "# Cr√©ez un DataFrame pour documenter vos r√©sultats\n",
        "\n",
        "granularite_analysis = []\n",
        "\n",
        "# Exemple pour dim_gare\n",
        "granularite_analysis.append({\n",
        "    \"table\": \"dim_gare\",\n",
        "    \"type\": \"dimension\",\n",
        "    \"granularite\": \"1 ligne = 1 gare\",\n",
        "    \"cle_primaire\": \"id_gares\",\n",
        "    \"nb_lignes\": 0,  # √Ä compl√©ter\n",
        "    \"colonnes_dimension\": \"aucune\"\n",
        "})\n",
        "\n",
        "# TODO : Ajoutez les autres tables\n",
        "# - dim_ligne\n",
        "# - dim_arret\n",
        "# - dim_vacances_scolaires\n",
        "# - dim_transporteur\n",
        "# - fact_validations_* (analyser au moins une table de validation)\n",
        "\n",
        "df_granularite = pd.DataFrame(granularite_analysis)\n",
        "display(df_granularite)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## T√¢che 2 : Identifier les Transformations N√©cessaires\n",
        "\n",
        "### Objectif\n",
        "\n",
        "Identifier les **transformations** √† appliquer aux donn√©es de la couche silver pour cr√©er la couche gold optimis√©e pour l'analyse.\n",
        "\n",
        "### Types de Transformations Possibles\n",
        "\n",
        "1. **Nettoyage des donn√©es**\n",
        "   - Suppression des doublons\n",
        "   - Gestion des valeurs NULL\n",
        "   - Normalisation des formats (dates, textes)\n",
        "\n",
        "2. **Enrichissement**\n",
        "   - Ajout de colonnes calcul√©es\n",
        "   - Jointures avec les tables de dimension\n",
        "   - Ajout de cat√©gories/segments\n",
        "\n",
        "3. **Agr√©gation**\n",
        "   - Regroupement par dimensions (jour, gare, ligne, etc.)\n",
        "   - Calcul de m√©triques (somme, moyenne, comptage)\n",
        "   - Cr√©ation de tables pr√©-agr√©g√©es\n",
        "\n",
        "4. **Normalisation**\n",
        "   - Uniformisation des formats de dates\n",
        "   - Standardisation des noms de colonnes\n",
        "   - Conversion de types de donn√©es\n",
        "\n",
        "5. **D√©duplication**\n",
        "   - Identification et suppression des doublons\n",
        "   - Consolidation de donn√©es dupliqu√©es\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Pour chaque table, identifiez :\n",
        "1. **Les probl√®mes de qualit√©** √† corriger\n",
        "2. **Les transformations n√©cessaires** avec des exemples concrets\n",
        "3. **Les colonnes √† ajouter** (calcul√©es ou issues de jointures)\n",
        "4. **Les agr√©gations possibles** pour optimiser les requ√™tes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple 1 : V√©rifier les doublons dans fact_validations\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    JOUR,\n",
        "    ID_ZDC,\n",
        "    CATEGORIE_TITRE,\n",
        "    COUNT(*) as nb_occurrences\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt`\n",
        "GROUP BY JOUR, ID_ZDC, CATEGORIE_TITRE\n",
        "HAVING COUNT(*) > 1\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "df_duplicates = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : Recherche de doublons ===\")\n",
        "display(df_duplicates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple 2 : V√©rifier les valeurs NULL\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    COUNT(*) as total_lignes,\n",
        "    COUNT(JOUR) as jour_not_null,\n",
        "    COUNT(ID_ZDC) as id_zdc_not_null,\n",
        "    COUNT(NB_VALD) as nb_vald_not_null,\n",
        "    COUNT(*) - COUNT(JOUR) as jour_null,\n",
        "    COUNT(*) - COUNT(ID_ZDC) as id_zdc_null,\n",
        "    COUNT(*) - COUNT(NB_VALD) as nb_vald_null\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt`\n",
        "\"\"\"\n",
        "\n",
        "df_nulls = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : Analyse des valeurs NULL ===\")\n",
        "display(df_nulls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Documentez les transformations n√©cessaires\n",
        "\n",
        "transformations = []\n",
        "\n",
        "# Exemple pour fact_validations\n",
        "transformations.append({\n",
        "    \"table_source\": \"fact_validations_*\",\n",
        "    \"probleme\": \"Format de date en STRING (DD/MM/YYYY)\",\n",
        "    \"transformation\": \"Convertir JOUR en DATE avec PARSE_DATE('%d/%m/%Y', JOUR)\",\n",
        "    \"exemple_sql\": \"SELECT PARSE_DATE('%d/%m/%Y', JOUR) as date_validation FROM ...\",\n",
        "    \"priorite\": \"Haute\"\n",
        "})\n",
        "\n",
        "transformations.append({\n",
        "    \"table_source\": \"fact_validations_*\",\n",
        "    \"probleme\": \"Colonnes ID_ZDC, lda, ID_REFA_LDA non normalis√©es\",\n",
        "    \"transformation\": \"Unifier les noms de colonnes (toutes ‚Üí id_zdc)\",\n",
        "    \"exemple_sql\": \"SELECT COALESCE(ID_ZDC, lda, ID_REFA_LDA) as id_zdc FROM ...\",\n",
        "    \"priorite\": \"Haute\"\n",
        "})\n",
        "\n",
        "# TODO : Ajoutez d'autres transformations identifi√©es\n",
        "# - Conversion de types (STRING ‚Üí INTEGER pour NB_VALD)\n",
        "# - Normalisation des noms de colonnes\n",
        "# - Ajout de colonnes calcul√©es (jour_semaine, est_vacances, etc.)\n",
        "# - Unification des tables fact_validations_*\n",
        "\n",
        "df_transformations = pd.DataFrame(transformations)\n",
        "display(df_transformations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## T√¢che 3 : Identifier les Cl√©s de Jointure Possibles\n",
        "\n",
        "### Objectif\n",
        "\n",
        "Identifier toutes les **relations possibles** entre les tables pour pouvoir cr√©er des jointures dans la couche gold.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Pour chaque paire de tables, identifiez :\n",
        "\n",
        "1. **Les colonnes de jointure** (cl√©s √©trang√®res)\n",
        "2. **Le type de relation** (1-1, 1-N, N-N)\n",
        "3. **La cardinalit√©** (combien de lignes de la table A correspondent √† combien de lignes de la table B)\n",
        "4. **V√©rifier l'int√©grit√© r√©f√©rentielle** (toutes les cl√©s √©trang√®res existent-elles dans la table de dimension ?)\n",
        "\n",
        "### Exemples de Relations √† Identifier\n",
        "\n",
        "- `fact_validations` ‚Üî `dim_gare` : via `ID_ZDC` ou `id_gares`\n",
        "- `fact_validations` ‚Üî `dim_ligne` : via `CODE_STIF_TRNS` ou `idrefliga`\n",
        "- `fact_validations` ‚Üî `dim_arret` : via `CODE_STIF_ARRET`\n",
        "- `dim_gare` ‚Üî `dim_ligne` : via `idrefliga`\n",
        "- `dim_ligne` ‚Üî `dim_transporteur` : via un code transporteur\n",
        "- `fact_validations` ‚Üî `dim_vacances_scolaires` : via la date\n",
        "- `fact_validations` ‚Üî `dim_jours_feries` : via la date (si la table existe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple 1 : V√©rifier la jointure fact_validations ‚Üî dim_gare\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    COUNT(DISTINCT v.ID_ZDC) as nb_id_zdc_distincts_fact,\n",
        "    COUNT(DISTINCT g.id_gares) as nb_id_gares_distincts_dim,\n",
        "    COUNT(DISTINCT v.ID_ZDC) - COUNT(DISTINCT g.id_gares) as difference\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt` v\n",
        "LEFT JOIN `{PROJECT_ID}.{DATASET_ID}.dim_gare` g\n",
        "    ON CAST(v.ID_ZDC AS INT64) = g.id_gares\n",
        "\"\"\"\n",
        "\n",
        "df_join_check = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : V√©rification de la jointure fact_validations ‚Üî dim_gare ===\")\n",
        "display(df_join_check)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple 2 : Identifier les cl√©s orphelines (cl√©s √©trang√®res sans correspondance)\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    v.ID_ZDC,\n",
        "    COUNT(*) as nb_occurrences\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt` v\n",
        "LEFT JOIN `{PROJECT_ID}.{DATASET_ID}.dim_gare` g\n",
        "    ON CAST(v.ID_ZDC AS INT64) = g.id_gares\n",
        "WHERE g.id_gares IS NULL\n",
        "GROUP BY v.ID_ZDC\n",
        "ORDER BY nb_occurrences DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "df_orphans = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : Cl√©s orphelines (sans correspondance dans dim_gare) ===\")\n",
        "display(df_orphans)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Documentez toutes les relations identifi√©es\n",
        "\n",
        "jointures = []\n",
        "\n",
        "# Exemple 1\n",
        "jointures.append({\n",
        "    \"table_source\": \"fact_validations\",\n",
        "    \"table_cible\": \"dim_gare\",\n",
        "    \"colonne_source\": \"ID_ZDC\",\n",
        "    \"colonne_cible\": \"id_gares\",\n",
        "    \"type_relation\": \"N-1\",\n",
        "    \"description\": \"Plusieurs validations peuvent √™tre associ√©es √† une m√™me gare\",\n",
        "    \"integrite\": \"√Ä v√©rifier\"\n",
        "})\n",
        "\n",
        "# Exemple 2\n",
        "jointures.append({\n",
        "    \"table_source\": \"fact_validations\",\n",
        "    \"table_cible\": \"dim_ligne\",\n",
        "    \"colonne_source\": \"CODE_STIF_TRNS\",\n",
        "    \"colonne_cible\": \"√Ä identifier\",\n",
        "    \"type_relation\": \"N-1\",\n",
        "    \"description\": \"Plusieurs validations peuvent √™tre associ√©es √† une m√™me ligne\",\n",
        "    \"integrite\": \"√Ä v√©rifier\"\n",
        "})\n",
        "\n",
        "# TODO : Ajoutez toutes les autres relations identifi√©es\n",
        "# - fact_validations ‚Üî dim_arret\n",
        "# - fact_validations ‚Üî dim_vacances_scolaires (via date)\n",
        "# - fact_validations ‚Üî dim_jours_feries (via date, si existe)\n",
        "# - dim_gare ‚Üî dim_ligne\n",
        "# - dim_ligne ‚Üî dim_transporteur\n",
        "# - etc.\n",
        "\n",
        "df_jointures = pd.DataFrame(jointures)\n",
        "display(df_jointures)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## T√¢che 4 : Autres Analyses et Recommandations\n",
        "\n",
        "### Objectif\n",
        "\n",
        "Effectuer des analyses compl√©mentaires pour optimiser la construction de la couche gold.\n",
        "\n",
        "### T√¢ches Sugg√©r√©es\n",
        "\n",
        "#### 4.1 - Analyse de la Qualit√© des Donn√©es\n",
        "\n",
        "- **Compl√©tude** : Pourcentage de valeurs non NULL par colonne\n",
        "- **Coh√©rence** : V√©rifier les valeurs aberrantes (ex: dates futures, nombres n√©gatifs)\n",
        "- **Unicit√©** : Identifier les colonnes qui devraient √™tre uniques mais ne le sont pas\n",
        "- **Validit√©** : V√©rifier que les valeurs respectent les contraintes m√©tier\n",
        "\n",
        "#### 4.2 - Analyse des Agr√©gations Possibles\n",
        "\n",
        "Identifier les **niveaux d'agr√©gation** utiles pour les analyses :\n",
        "- Par jour, semaine, mois, ann√©e\n",
        "- Par gare, ligne, arr√™t, transporteur\n",
        "- Par type de titre (cat√©gorie)\n",
        "- Combinaisons de ces dimensions\n",
        "\n",
        "#### 4.3 - Analyse de Performance\n",
        "\n",
        "- **Volume de donn√©es** : Taille des tables, nombre de lignes\n",
        "- **Fr√©quence d'utilisation** : Quelles tables seront les plus sollicit√©es ?\n",
        "- **Optimisations possibles** : Partitionnement, clustering, tables mat√©rialis√©es\n",
        "\n",
        "#### 4.4 - Identification des M√©triques M√©tier\n",
        "\n",
        "Identifier les **KPIs** (Key Performance Indicators) √† calculer :\n",
        "- Nombre total de validations par p√©riode\n",
        "- Taux de croissance des validations\n",
        "- R√©partition par type de titre\n",
        "- Top 10 des gares les plus fr√©quent√©es\n",
        "- Comparaison jour ouvrable vs jour f√©ri√© vs vacances\n",
        "\n",
        "#### 4.5 - Proposition de Sch√©ma Gold\n",
        "\n",
        "Proposer une **structure de tables** pour la couche gold :\n",
        "- Tables de fait agr√©g√©es (ex: `fact_validations_daily`)\n",
        "- Tables de dimension enrichies (ex: `dim_gare_enriched`)\n",
        "- Tables de m√©triques pr√©-calcul√©es (ex: `metrics_affluence_weekly`)\n",
        "- Vues pour faciliter les requ√™tes courantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple : Analyse de compl√©tude des donn√©es\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    'fact_validations_2023_s2_nb_fer_txt' as table_name,\n",
        "    COUNT(*) as total_lignes,\n",
        "    ROUND(100.0 * COUNT(JOUR) / COUNT(*), 2) as pct_jour_not_null,\n",
        "    ROUND(100.0 * COUNT(ID_ZDC) / COUNT(*), 2) as pct_id_zdc_not_null,\n",
        "    ROUND(100.0 * COUNT(NB_VALD) / COUNT(*), 2) as pct_nb_vald_not_null,\n",
        "    ROUND(100.0 * COUNT(CATEGORIE_TITRE) / COUNT(*), 2) as pct_categorie_not_null\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt`\n",
        "\"\"\"\n",
        "\n",
        "df_quality = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : Analyse de compl√©tude ===\")\n",
        "display(df_quality)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple : Identifier les niveaux d'agr√©gation possibles\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    COUNT(DISTINCT JOUR) as nb_jours_distincts,\n",
        "    COUNT(DISTINCT ID_ZDC) as nb_gares_distincts,\n",
        "    COUNT(DISTINCT CODE_STIF_TRNS) as nb_lignes_distincts,\n",
        "    COUNT(DISTINCT CATEGORIE_TITRE) as nb_categories_distincts,\n",
        "    COUNT(*) as total_validations\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt`\n",
        "\"\"\"\n",
        "\n",
        "df_aggregations = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : Dimensions disponibles pour agr√©gation ===\")\n",
        "display(df_aggregations)\n",
        "\n",
        "print(\"\\n=== Niveaux d'agr√©gation possibles ===\")\n",
        "print(\"- Par jour : ~\", df_aggregations['nb_jours_distincts'].iloc[0], \"combinaisons\")\n",
        "print(\"- Par jour √ó gare : ~\", df_aggregations['nb_jours_distincts'].iloc[0] * df_aggregations['nb_gares_distincts'].iloc[0], \"combinaisons\")\n",
        "print(\"- Par jour √ó gare √ó cat√©gorie : ~\", df_aggregations['nb_jours_distincts'].iloc[0] * df_aggregations['nb_gares_distincts'].iloc[0] * df_aggregations['nb_categories_distincts'].iloc[0], \"combinaisons\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple : Calculer des m√©triques m√©tier\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    PARSE_DATE('%d/%m/%Y', JOUR) as date_validation,\n",
        "    COUNT(DISTINCT ID_ZDC) as nb_gares,\n",
        "    SUM(CAST(NB_VALD AS INT64)) as total_validations,\n",
        "    AVG(CAST(NB_VALD AS FLOAT64)) as moyenne_validations_par_ligne\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_validations_2023_s2_nb_fer_txt`\n",
        "WHERE JOUR IS NOT NULL AND NB_VALD IS NOT NULL\n",
        "GROUP BY date_validation\n",
        "ORDER BY date_validation\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "df_metrics = bq_client.query(query).to_dataframe()\n",
        "print(\"=== Exemple : M√©triques m√©tier (validations par jour) ===\")\n",
        "display(df_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### √Ä Compl√©ter : Recommandations pour la Couche Gold\n",
        "\n",
        "**Votre t√¢che :** Remplissez ce tableau avec vos recommandations bas√©es sur vos analyses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO : Documentez vos recommandations\n",
        "\n",
        "recommandations = []\n",
        "\n",
        "# Exemple 1\n",
        "recommandations.append({\n",
        "    \"categorie\": \"Transformation\",\n",
        "    \"recommandation\": \"Unifier toutes les tables fact_validations_* en une seule table\",\n",
        "    \"justification\": \"Faciliter les requ√™tes et analyses cross-ann√©es\",\n",
        "    \"priorite\": \"Haute\"\n",
        "})\n",
        "\n",
        "# Exemple 2\n",
        "recommandations.append({\n",
        "    \"categorie\": \"Agr√©gation\",\n",
        "    \"recommandation\": \"Cr√©er une table fact_validations_daily agr√©g√©e par jour √ó gare √ó cat√©gorie\",\n",
        "    \"justification\": \"R√©duire le volume de donn√©es et acc√©l√©rer les requ√™tes d'analyse\",\n",
        "    \"priorite\": \"Moyenne\"\n",
        "})\n",
        "\n",
        "# Exemple 3\n",
        "recommandations.append({\n",
        "    \"categorie\": \"Enrichissement\",\n",
        "    \"recommandation\": \"Ajouter des colonnes calcul√©es : jour_semaine, est_vacances, est_ferie\",\n",
        "    \"justification\": \"Faciliter les analyses comparatives (weekend vs semaine, vacances vs p√©riode scolaire)\",\n",
        "    \"priorite\": \"Haute\"\n",
        "})\n",
        "\n",
        "# TODO : Ajoutez vos propres recommandations\n",
        "# - Optimisations de performance (partitionnement, clustering)\n",
        "# - Nettoyage de donn√©es\n",
        "# - Cr√©ation de vues\n",
        "# - Tables de m√©triques pr√©-calcul√©es\n",
        "# - etc.\n",
        "\n",
        "df_recommandations = pd.DataFrame(recommandations)\n",
        "display(df_recommandations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion et Prochaines √âtapes\n",
        "\n",
        "### R√©sum√© de Votre Travail\n",
        "\n",
        "√Ä la fin de ce notebook, vous devriez avoir produit :\n",
        "\n",
        "1. ‚úÖ **Analyse de granularit√©** : Documentation compl√®te de chaque table\n",
        "2. ‚úÖ **Liste des transformations** : Transformations n√©cessaires avec exemples SQL\n",
        "3. ‚úÖ **Sch√©ma de jointures** : Toutes les relations identifi√©es entre les tables\n",
        "4. ‚úÖ **Recommandations** : Propositions pour optimiser la couche gold\n",
        "\n",
        "### Prochaines √âtapes\n",
        "\n",
        "Une fois ces analyses termin√©es, vous pourrez :\n",
        "\n",
        "1. **Cr√©er le notebook de transformation** (`4_[TRANSFORM]_build_gold.ipynb`)\n",
        "2. **Impl√©menter les transformations identifi√©es**\n",
        "3. **Cr√©er les tables de la couche gold** dans BigQuery\n",
        "4. **Valider la qualit√© des donn√©es** transform√©es\n",
        "5. **Cr√©er des vues** pour faciliter les requ√™tes d'analyse\n",
        "\n",
        "### Ressources Utiles\n",
        "\n",
        "- [Documentation BigQuery SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)\n",
        "- [Fonctions de date BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/date_functions)\n",
        "- [Fonctions d'agr√©gation BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/aggregate_functions)\n",
        "- [Partitionnement et clustering BigQuery](https://cloud.google.com/bigquery/docs/partitioned-tables)\n",
        "\n",
        "---\n",
        "\n",
        "**Bon travail ! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
